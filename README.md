# YOLOv2-PyTorch

<div align="center">

**A clean, modular, and production-ready implementation of YOLOv2 object detection in PyTorch**

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.10+](https://img.shields.io/badge/pytorch-1.10+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

> **About**: This is a comprehensive implementation of YOLOv2 (YOLO9000) featuring the Darknet-19 backbone, anchor box-based detection, and modern training infrastructure. Built with clean, modular code and complete evaluation tools for research and production use.

---

## üéØ Features

### ‚ú® **Modern & Clean Architecture**
- üèóÔ∏è **Modular Design**: Clean separation of concerns (models, data, utils)
- üîÑ **Modern PyTorch**: Latest API with best practices and optimizations
- üì¶ **Easy Installation**: Standard Python package with dependency management
- üìù **Type Hints**: Full type annotations for better IDE support and code quality
- üß™ **Testable**: Individual component testing support

### üöÄ **State-of-the-Art Detection**
- ‚ö° **Darknet-19 Backbone**: Efficient 19-layer convolutional feature extractor (~50M params)
- üéØ **Anchor Boxes**: 5 K-means clustered anchors for robust multi-scale detection
- üîó **Passthrough Layer**: Fine-grained feature fusion for improved small object detection
- üíØ **Batch Normalization**: All convolutional layers use BN for training stability
- üé™ **Multi-Scale Training**: Support for various input resolutions (320-640px)

### üõ†Ô∏è **Production-Ready Training**
- üìä **Ultralytics Compatibility**: Drop-in support for YAML+TXT dataset format (Roboflow, COCO, etc.)
- üîß **Highly Configurable**: Extensive CLI arguments and YAML-based configuration
- üìà **Complete Evaluation Suite**: mAP@0.5, Precision, Recall, F1-Score (fast evaluation)
- üé® **Rich Visualization**: Training curves, confusion matrices, PR curves, and prediction samples
- üîÑ **Training Stability**: Gradient clipping, EMA-smoothed validation, AdamW optimizer with cosine annealing
- üíæ **Smart Checkpointing**: Auto-save best models based on mAP or validation loss
- üìâ **Real-Time Monitoring**: Live progress bars with detailed loss component tracking

### üéõÔ∏è **Advanced Fine-tuning**
- üìâ **ReduceLROnPlateau**: Auto-reduce learning rate when validation loss plateaus
- üõë **Early Stopping**: Stop training when no improvement after multiple LR reductions
- ‚ö° **Mixed Precision (AMP)**: 2x faster training with half memory usage
- üì¶ **Gradient Accumulation**: Simulate larger batch sizes on limited GPU memory
- üîÑ **Model EMA**: Exponential Moving Average for more stable inference
- üè∑Ô∏è **Label Smoothing**: Prevent overconfidence and improve generalization

---

## üìÅ Project Structure

```
./
‚îú‚îÄ‚îÄ yolov2/                      # Core package
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Model definitions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ darknet.py          # Darknet-19 backbone
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolov2.py           # YOLOv2 detection network
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ layers.py           # Custom layers (ConvBNAct, SpaceToDepth)
‚îÇ   ‚îú‚îÄ‚îÄ data/                    # Data processing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datasets.py         # COCODetectionDataset (Ultralytics format)
‚îÇ   ‚îî‚îÄ‚îÄ utils/                   # Utility functions
‚îÇ       ‚îú‚îÄ‚îÄ loss.py             # YOLOv2 loss function
‚îÇ       ‚îú‚îÄ‚îÄ general.py          # NMS, IoU, etc.
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py          # Evaluation metrics (mAP, Precision, Recall)
‚îÇ       ‚îú‚îÄ‚îÄ plots.py            # Visualization tools (PR curves, confusion matrix)
‚îÇ       ‚îî‚îÄ‚îÄ callbacks.py        # Training callbacks (EarlyStopping, LR scheduler, EMA)
‚îú‚îÄ‚îÄ scripts/                     # Training & inference scripts
‚îÇ   ‚îú‚îÄ‚îÄ train.py                # Training script with full evaluation
‚îÇ   ‚îî‚îÄ‚îÄ detect.py               # Detection/inference script
‚îú‚îÄ‚îÄ runs/                        # Training outputs
‚îÇ   ‚îî‚îÄ‚îÄ train/                  # Training experiments
‚îÇ       ‚îî‚îÄ‚îÄ exp/                # Experiment results
‚îÇ           ‚îú‚îÄ‚îÄ weights/        # Model checkpoints (best.pt, last.pt)
‚îÇ           ‚îî‚îÄ‚îÄ *.png           # Training plots and visualizations
‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies
‚îú‚îÄ‚îÄ setup.py                     # Package setup
‚îî‚îÄ‚îÄ README.md                    # This file
```

---

## üöÄ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/seagochen/yolov2-pytorch
cd yolov2-pytorch

# Install dependencies
pip install -r requirements.txt

# Install as editable package (recommended for development)
pip install -e .
```

**Requirements:**
- Python >= 3.7
- PyTorch >= 1.10.0
- torchvision >= 0.11.0
- NumPy >= 1.19.0
- PyYAML >= 5.4.0
- tqdm >= 4.60.0
- matplotlib >= 3.3.0
- CUDA (recommended for GPU training)

### Training

```bash
python scripts/train.py \
    --data /path/to/your/dataset.yaml \
    --epochs 100 \
    --batch-size 16 \
    --img-size 640 \
    --device 0
```

**Training Arguments:**
```
--data          Dataset YAML config path (required)
--epochs        Number of training epochs (default: 100)
--batch-size    Batch size (default: 16)
--img-size      Input image size (default: 640)
--lr            Initial learning rate (default: 5e-4)
--weight-decay  Optimizer weight decay (default: 5e-4)
--device        CUDA device, i.e. 0 or 0,1,2,3 or cpu
--workers       Number of data loading workers (default: 4)
--project       Save directory (default: runs/train)
--name          Experiment name (default: exp)
--resume        Resume from experiment name (e.g. "exp_2", auto-finds last.pt)
--grad-clip     Gradient clipping threshold (default: 10.0)
--warmup-epochs Warmup epochs before cosine annealing (default: 3)
```

**Fine-tuning Arguments:**
```
--patience            Epochs without improvement before LR reduction (default: 5)
--lr-factor           Factor to reduce learning rate (default: 0.1)
--min-lr              Minimum learning rate (default: 1e-7)
--early-stopping      Enable early stopping
--max-lr-reductions   Max LR reductions before early stopping (default: 3)
--amp                 Use Automatic Mixed Precision (FP16) training
--accumulation-steps  Gradient accumulation steps (default: 1)
--ema                 Use Exponential Moving Average for model weights
--ema-decay           EMA decay factor (default: 0.9999)
--label-smoothing     Label smoothing factor (default: 0.0)
--eval-interval       Compute full metrics every N epochs (default: 5, 0=only last)
```

**Training Outputs:**
The training script generates comprehensive outputs in `runs/train/exp/`:
- `weights/best.pt` - Best model checkpoint (highest mAP)
- `weights/last.pt` - Last epoch checkpoint
- `training_curves.png` - Training metrics curves (loss, mAP, precision, recall)
- `metrics.csv` - Per-epoch metrics data (updated in real-time)
- `labels_distribution.png` - Label distribution visualization
- `val_batch_predictions.jpg` - Validation predictions with color-coded classes

### Detection

```bash
python scripts/detect.py \
    --weights runs/train/exp/weights/best.pt \
    --source path/to/images \
    --conf-thres 0.5 \
    --save-img
```

**Detection Arguments:**
```
--weights       Model weights path
--source        Source: image file, folder, or video
--conf-thres    Confidence threshold (default: 0.5)
--iou-thres     NMS IOU threshold (default: 0.5)
--img-size      Inference size (default: 640)
--save-img      Save detection results
--view-img      Display results
```

### Example Training Output

During training, you'll see real-time metrics:

```
Epoch 1/100:  100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [05:23<00:00,  3.09it/s]
Train - Loss: 12.345, Box: 4.123, Obj: 3.456, Cls: 4.766

Validating:  100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:45<00:00,  4.41it/s]
Val - mAP@0.5: 0.523, P: 0.645, R: 0.589, F1: 0.616

‚úì Best mAP improved from 0.000 to 0.523, saving best.pt...
```

Training automatically generates visualization plots showing:
- Loss curves (box, objectness, class losses)
- mAP progression over epochs
- Precision and Recall curves
- Confusion matrix for error analysis

---

## üìä Dataset Format

### Ultralytics YAML Format

YOLOv2-PyTorch fully supports the Ultralytics dataset format:

**YAML Configuration** (`data/coco.yaml`):
```yaml
# Dataset root directory
path: /path/to/dataset

# Dataset splits
train: images/train
val: images/val
test: images/test  # optional

# Number of classes
nc: 80

# Class names
names:
  - person
  - bicycle
  - car
  # ... (80 classes total)
```

**Directory Structure:**
```
dataset/
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ val/
‚îÇ       ‚îú‚îÄ‚îÄ img001.jpg
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ labels/
    ‚îú‚îÄ‚îÄ train/
    ‚îÇ   ‚îú‚îÄ‚îÄ img001.txt
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ val/
        ‚îú‚îÄ‚îÄ img001.txt
        ‚îî‚îÄ‚îÄ ...
```

**TXT Annotation Format** (one object per line):
```
class_id center_x center_y width height
```
- All coordinates normalized to [0, 1]
- `class_id`: 0-based integer
- `center_x, center_y`: Center point of bounding box
- `width, height`: Box dimensions

**Example** (`img001.txt`):
```
0 0.5 0.5 0.3 0.4    # person at image center
2 0.2 0.3 0.15 0.2   # car in top-left
```

---

## üèóÔ∏è Architecture

### YOLOv2 Network

```
Input: (B, 3, 640, 640)
  ‚Üì
[Darknet-19 Backbone]
  ‚îú‚îÄ Block1: 640‚Üí160 (Conv + Pool)
  ‚îú‚îÄ Block2: 160‚Üí80
  ‚îú‚îÄ Block3: 80‚Üí40
  ‚îú‚îÄ Block4: 40‚Üí20 ‚Üí [passthrough: 40√ó40√ó512]
  ‚îî‚îÄ Block5: 20√ó20√ó1024
  ‚Üì
[Passthrough Layer]
  40√ó40√ó512 ‚Üí SpaceToDepth ‚Üí 20√ó20√ó2048 ‚Üí Conv1√ó1 ‚Üí 20√ó20√ó64
  ‚Üì
[Concat]
  [20√ó20√ó1024, 20√ó20√ó64] ‚Üí 20√ó20√ó1088
  ‚Üì
[Detection Head]
  Conv3√ó3 ‚Üí Conv1√ó1 ‚Üí 20√ó20√ó(5√ó(5+80))
  ‚Üì
Output: (B, 5, 20, 20, 85)
  where 85 = 5 (tx,ty,tw,th,conf) + 80 (classes)
```

### Anchor Boxes

5 pre-defined anchors (from K-means clustering on COCO):
```python
anchors = [
    [0.57273, 0.677385],   # Small objects
    [1.87446, 2.06253],    # Medium objects
    [3.33843, 5.47434],    # Large objects
    [7.88282, 3.52778],    # Wide objects
    [9.77052, 9.16828]     # Very large objects
]
```

---

## üîß Advanced Usage

### Custom Dataset

1. **Prepare data** in Ultralytics format (see Dataset Format section)
2. **Create YAML** config file with paths and class names
3. **Train**:
   ```bash
   python scripts/train.py --data path/to/custom.yaml --epochs 100
   ```

### Fine-tuning Examples

**Basic training with early stopping:**
```bash
# Stop if validation loss doesn't improve for 5 epochs √ó 3 LR reductions
python scripts/train.py --data data/coco.yaml \
    --patience 5 \
    --early-stopping \
    --max-lr-reductions 3
```

**Full fine-tuning suite (recommended):**
```bash
python scripts/train.py --data data/coco.yaml \
    --patience 5 \
    --early-stopping \
    --max-lr-reductions 3 \
    --amp \
    --ema \
    --label-smoothing 0.1
```

**Low GPU memory scenario:**
```bash
# Use gradient accumulation to simulate batch_size=64
python scripts/train.py --data data/coco.yaml \
    --batch-size 16 \
    --accumulation-steps 4 \
    --amp
```

**Fine-tuning workflow:**
1. **Warmup phase** (first 3 epochs): LR linearly increases
2. **Normal training**: Cosine annealing + ReduceLROnPlateau monitors val loss
3. **LR reduction**: If no improvement for `patience` epochs, LR √ó `lr-factor`
4. **Early stop**: After `max-lr-reductions` LR reductions without improvement

### Evaluation Metrics

The training script automatically computes:
- **mAP@0.5**: Mean Average Precision at IoU threshold 0.5
- **F1-Score**: Harmonic mean of precision and recall
- **Precision & Recall**: Per-class and overall metrics
- **Confusion Matrix**: Visual analysis of predictions vs ground truth
- **PR Curves**: Precision-Recall curves for each class

### Export Model

```python
import torch
from yolov2 import create_yolov2

model = create_yolov2(num_classes=80, img_size=640)
model.load_state_dict(torch.load('best.pt')['model'])

# Export to TorchScript
traced = torch.jit.trace(model, torch.randn(1, 3, 640, 640))
traced.save('yolov2.torchscript')

# Export to ONNX
torch.onnx.export(
    model,
    torch.randn(1, 3, 640, 640),
    'yolov2.onnx',
    input_names=['images'],
    output_names=['predictions']
)
```

---

## üìà Performance

### Model Statistics

| Metric | Value |
|--------|-------|
| **Parameters** | ~50M |
| **Model Size** | ~200 MB |
| **FLOPs** | ~34B |
| **Inference Speed** | ~40 FPS (RTX 3090) |

### Expected Performance (COCO)

| Metric | Value |
|--------|-------|
| **mAP@0.5** | ~68% @ 640√ó640 |
| **F1-Score** | ~0.65 |

---

## üéì Key Improvements Over YOLOv1

| Feature | YOLOv1 | YOLOv2 |
|---------|--------|--------|
| **Backbone** | Custom CNN | **Darknet-19** |
| **Anchor Boxes** | ‚ùå Direct regression | **‚úÖ 5 anchors** |
| **Passthrough** | ‚ùå No | **‚úÖ Yes** (fine-grained features) |
| **Batch Norm** | Partial | **‚úÖ All layers** |
| **Fully Convolutional** | ‚ùå Uses FC layers | **‚úÖ Pure conv** |
| **Parameters** | ~100M | **~50M** (50% reduction) |
| **Small Object Detection** | Poor | **Good** |

---

## üõ†Ô∏è Development

### Package Structure

The project is organized as a Python package:
- `yolov2/models/`: Neural network architectures
- `yolov2/data/`: Dataset loading and preprocessing
- `yolov2/utils/`: Loss functions, metrics, plotting, and utilities

### Testing Individual Components

```bash
# Test model architecture
python -m yolov2.models.yolov2

# Test dataset loading
python -m yolov2.data.datasets

# Test loss computation
python -m yolov2.utils.loss
```

---

## üìö References

- **YOLO9000: Better, Faster, Stronger**
  Joseph Redmon, Ali Farhadi
  [arXiv:1612.08242](https://arxiv.org/abs/1612.08242)

- **You Only Look Once: Unified, Real-Time Object Detection**
  Joseph Redmon et al.
  [arXiv:1506.02640](https://arxiv.org/abs/1506.02640)

- **Darknet Framework**
  [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)

---

## üìù Recent Updates

### v1.4 - Metrics & Visualization Improvements (2025-11)
- ‚úÖ **Fast Evaluation**: Only compute mAP@0.5 (10x faster than mAP@0.5:0.95)
- ‚úÖ **Periodic Metrics Evaluation**: New `--eval-interval` to compute mAP every N epochs
- ‚úÖ **Real-time CSV Export**: Metrics saved after each epoch, preventing data loss on interruption
- ‚úÖ **Color-coded Visualization**: Different colors for each class in detection plots
- ‚úÖ **Fixed mAP Calculation**: Correct target bbox decoding for accurate metrics
- ‚úÖ **Improved Early Stopping**: Compute full metrics before stopping for final evaluation

### v1.3 - Advanced Fine-tuning Suite (2025-11)
- ‚úÖ **ReduceLROnPlateau**: Auto-reduce LR when val loss stops improving (patience=5)
- ‚úÖ **Early Stopping**: Stop training after N consecutive LR reductions without improvement
- ‚úÖ **Mixed Precision (AMP)**: FP16 training for 2x speedup and 50% memory reduction
- ‚úÖ **Gradient Accumulation**: Simulate larger batch sizes on limited GPU memory
- ‚úÖ **Model EMA**: Exponential Moving Average of model weights for stable inference
- ‚úÖ **Label Smoothing**: Prevent overconfidence and improve generalization
- ‚úÖ **Checkpoint State**: Save/restore all fine-tuning component states for seamless resume

### v1.2 - Training Stability Improvements (2025-01)
- ‚úÖ **Gradient Clipping**: Prevent gradient explosion during training
- ‚úÖ **AdamW Optimizer**: Improved weight decay regularization
- ‚úÖ **Cosine Annealing**: Smooth learning rate scheduling with warmup
- ‚úÖ **BCEWithLogitsLoss**: Better numerical stability in loss computation
- ‚úÖ **EMA Validation Loss**: Robust model selection via exponential moving average
- ‚úÖ **Memory Optimization**: torch.no_grad() wrapper for validation
- ‚úÖ **Roboflow Support**: Fixed dataset path handling for Roboflow exports
- ‚úÖ **Smart Validation**: Full metrics only on final epoch to save time

### v1.1 - Comprehensive Training Evaluation System (2024-11)
- ‚úÖ Full evaluation metrics (mAP@0.5, Precision, Recall, F1)
- ‚úÖ Ultralytics-style training visualization and reporting
- ‚úÖ Confusion matrix and PR curve plotting
- ‚úÖ Per-class and overall detection metrics tracking
- ‚úÖ Organized training output with automatic result saving

### v1.0 - Production-Ready YOLOv2 (2024-11)
- ‚úÖ Complete YOLOv2 implementation with Darknet-19 backbone
- ‚úÖ Anchor box-based multi-scale detection system
- ‚úÖ Ultralytics YAML+TXT dataset format support
- ‚úÖ Modular architecture with clean, maintainable code

---

## ü§ù Contributing

Contributions are welcome! Here's how you can help:

1. **Report Bugs**: Open an issue with detailed reproduction steps
2. **Suggest Features**: Share your ideas for improvements
3. **Submit PRs**: Fix bugs, add features, or improve documentation
4. **Share Results**: Post your training results and model performance

Please ensure your code follows the existing style and includes appropriate tests.

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Joseph Redmon** and **Ali Farhadi** for the original YOLO and YOLOv2 papers
- **Ultralytics** for the standardized dataset format and training methodology
- **PyTorch Team** for the excellent deep learning framework
- **Darknet** project for the original implementation reference

---

<div align="center">

**‚≠ê If you find this project useful, please consider giving it a star! ‚≠ê**

</div>
